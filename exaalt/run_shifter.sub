#!/bin/bash
# Begin SLURM Directives
#SBATCH -C gpu
#SBATCH -t 00:45:00
#SBATCH -J LAMMPS_SNAP_shifter
#SBATCH -o LAMMPS_SNAP_shifter.o%j
#SBATCH -A nstaff_g
#SBATCH -n _NTASKS_
#SBATCH --cpus-per-task=32
#SBATCH --gpus-per-task=1
#SBATCH --gpu-bind=none
#SBATCH --image=registry.nersc.gov/das/exaalt:benchmark
##SBATCH --reservation=podman_gpu_4
#SBATCH -q early_science

module purge

export PMI_MMAP_SYNC_WAIT_TIME=300
export MPICH_GPU_SUPPORT_ENABLED=1

gpus_per_node=4
exe="/opt/LAMMPS_INSTALL/bin/lmp"
input="-pk kokkos newton on neigh half -k on g $gpus_per_node -sf kk -in in.run"
total_nodes=$(( _NTASKS_/4 ))

natoms=`echo "_NREP_*_NREP_*_NREP_*46656"|bc`
jobdir=$benchmark-natoms-$natoms-nodes-$total_nodes-job-$SLURM_JOB_ID
mkdir $jobdir
cp -r files/* $jobdir
cd $jobdir
sed -i 's/_N_/'_NREP_'/g' in.run

#turns out we don't need the cuda-mpich module, the ss11 mpich rpm has gtl already
command="srun shifter ./wrap.sh $exe $input -log log._NREP_"
echo $command
echo "first run"
$command
sleep 5
echo "second run"
$command
echo "done"



